{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALSI: Phi Projector Training (Phase 2)\n",
    "\n",
    "This notebook trains a non-linear projector (Phi) to perform Augmented Latent State Injection (ALSI) on Mamba-2.\n",
    "\n",
    "## 1. Setup Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate einops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "model_id = \"AntonV/mamba2-130m-hf\"\n",
    "LAYER_IDX = 7\n",
    "PROBE_TEXT = \"The password is '\"\n",
    "TARGETS = [\"BLUE\", \"RED\", \"GREEN\", \"ORANGE\", \"YELLOW\", \"BLACK\", \"WHITE\", \"PURPLE\", \"GOLD\", \"SILVER\"]\n",
    "DATA_DIR = \"./ALSI_data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Generation (Ground Truth Optimization)\n",
    "\n",
    "We first find the 'Golden Deltas' for each target via direct optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_control_delta(model, tokenizer, h_prev_cache, target_str, steps=200, lr=1.0):\n",
    "    target_id = tokenizer.encode(target_str, add_special_tokens=False)[0]\n",
    "    print(f\"Optimizing for target: {target_str} (ID: {target_id})\")\n",
    "    \n",
    "    base_state_shape = h_prev_cache.ssm_states[LAYER_IDX].shape\n",
    "    delta = torch.zeros(base_state_shape, device=model.device, requires_grad=True)\n",
    "    optimizer = torch.optim.Adam([delta], lr=lr)\n",
    "    \n",
    "    last_token_id = tokenizer(PROBE_TEXT, return_tensors=\"pt\").input_ids[:, -1:].to(device)\n",
    "    cache_pos = torch.tensor([tokenizer(PROBE_TEXT, return_tensors=\"pt\").input_ids.shape[1]-1], device=model.device)\n",
    "    \n",
    "    best_rank = 99999\n",
    "    \n",
    "    for step in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        base_states_tensor = h_prev_cache.ssm_states.detach()\n",
    "        layers_list = [base_states_tensor[i] for i in range(model.config.num_hidden_layers)]\n",
    "        layers_list[LAYER_IDX] = layers_list[LAYER_IDX] + delta\n",
    "        injected_ssm_states = torch.stack(layers_list)\n",
    "        \n",
    "        class MockCache:\n",
    "            def __init__(self, ssm, conv):\n",
    "                self.ssm_states = ssm\n",
    "                self.conv_states = conv\n",
    "                self.config = model.config\n",
    "                self.conv_kernel_size = model.config.conv_kernel\n",
    "            def update_ssm_state(self, layer_idx, new_ssm_state, cache_init=False): return\n",
    "            def update_conv_state(self, layer_idx, new_conv_state, cache_init=False): return\n",
    "            \n",
    "        diff_cache = MockCache(injected_ssm_states, h_prev_cache.conv_states)\n",
    "        outputs = model(last_token_id, cache_params=diff_cache, cache_position=cache_pos)\n",
    "        logits = outputs.logits[0, -1]\n",
    "        \n",
    "        # Cross Entropy Loss\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(1, -1), torch.tensor([target_id], device=model.device)) + 1e-5 * delta.norm()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            rank = (logits > logits[target_id]).sum().item() + 1\n",
    "            if rank < best_rank: best_rank = rank\n",
    "            if rank <= 5: return delta.detach().cpu(), target_id, True\n",
    "                \n",
    "    return delta.detach().cpu(), target_id, best_rank <= 10\n",
    "\n",
    "def generate_dataset(model, tokenizer):\n",
    "    probe_ids = tokenizer(PROBE_TEXT, return_tensors=\"pt\").input_ids.to(device)\n",
    "    context_ids = probe_ids[:, :-1]\n",
    "    with torch.no_grad():\n",
    "        out_prev = model(context_ids, use_cache=True)\n",
    "    h_prev_cache = out_prev.cache_params\n",
    "    \n",
    "    h_prev_tensor = h_prev_cache.ssm_states[LAYER_IDX].detach().cpu()\n",
    "    dataset = []\n",
    "    for target in TARGETS:\n",
    "        delta, t_id, success = optimize_control_delta(model, tokenizer, h_prev_cache, target)\n",
    "        if success:\n",
    "            dataset.append({\"h_prev\": h_prev_tensor, \"target_id\": t_id, \"delta\": delta, \"target_str\": target})\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Phi Projector Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhiProjector(nn.Module):\n",
    "    def __init__(self, state_dim, embed_dim, hidden_dim=1024):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, state_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, h_prev, target_embed):\n",
    "        x = torch.cat([h_prev, target_embed], dim=-1)\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Main Execution Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model AntonV/mamba2-130m-hf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59323a5109a1445f8001c37134dad24e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/756 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a660d1713846828a52f54f6ab7e3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/516M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The fast path is not available because one of `(selective_state_update, causal_conv1d_fn, causal_conv1d_update)` is None. Falling back to the naive implementation. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2491805a744242a9a732debffb2f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43ec95bbca94ec08087c32884bd7dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6baeba89498e412ab81babf82d7b1e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/587 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing for target: BLUE (ID: 5993)\n",
      "Optimizing for target: RED (ID: 34803)\n",
      "Optimizing for target: GREEN (ID: 41127)\n",
      "Optimizing for target: ORANGE (ID: 1372)\n",
      "Optimizing for target: YELLOW (ID: 27413)\n",
      "Optimizing for target: BLACK (ID: 5993)\n",
      "Optimizing for target: WHITE (ID: 8835)\n",
      "Optimizing for target: PURPLE (ID: 49)\n",
      "Optimizing for target: GOLD (ID: 40)\n",
      "Optimizing for target: SILVER (ID: 52)\n",
      "Generated 10 samples.\n",
      "Epoch 0: Loss 31.1053\n",
      "Epoch 20: Loss 10.9237\n",
      "Epoch 40: Loss 10.0749\n",
      "Epoch 60: Loss 9.8845\n",
      "Epoch 80: Loss 10.0150\n",
      "Epoch 100: Loss 9.6845\n",
      "Target: BLUE | Final Rank: 1\n",
      "Target: RED | Final Rank: 5\n",
      "Target: GREEN | Final Rank: 3\n",
      "Target: ORANGE | Final Rank: 2\n",
      "Target: YELLOW | Final Rank: 4\n",
      "Target: BLACK | Final Rank: 1\n",
      "Target: WHITE | Final Rank: 2\n",
      "Target: PURPLE | Final Rank: 3\n",
      "Target: GOLD | Final Rank: 5\n",
      "Target: SILVER | Final Rank: 5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading model {model_id}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# 1. Generate Dataset\n",
    "dataset = generate_dataset(model, tokenizer)\n",
    "print(f\"Generated {len(dataset)} samples.\")\n",
    "\n",
    "# 2. Initialize Phi\n",
    "embedding_layer = model.get_input_embeddings()\n",
    "sample_h = dataset[0]['h_prev']\n",
    "state_dim = sample_h.numel()\n",
    "embed_dim = embedding_layer.weight.shape[1]\n",
    "phi = PhiProjector(state_dim, embed_dim).to(device)\n",
    "optimizer = optim.Adam(phi.parameters(), lr=1e-4)\n",
    "\n",
    "# 3. Training Loop\n",
    "probe_ids = tokenizer(PROBE_TEXT, return_tensors=\"pt\").input_ids.to(device)\n",
    "context_ids = probe_ids[:, :-1]\n",
    "last_token_id = probe_ids[:, -1:]\n",
    "with torch.no_grad():\n",
    "    out_prev = model(context_ids, use_cache=True)\n",
    "base_cache_struct = out_prev.cache_params\n",
    "\n",
    "for epoch in range(101):\n",
    "    total_l = 0\n",
    "    for sample in dataset:\n",
    "        optimizer.zero_grad()\n",
    "        t_id = sample['target_id']\n",
    "        gt_delta = sample['delta'].to(device)\n",
    "        h_prev = sample['h_prev'].to(device).view(1, -1)\n",
    "        t_embed = embedding_layer(torch.tensor([[t_id]], device=device)).view(1, -1)\n",
    "        \n",
    "        pred_delta_flat = phi(h_prev, t_embed)\n",
    "        pred_delta = pred_delta_flat.view(sample_h.shape)\n",
    "        \n",
    "        # Match Loss\n",
    "        l_match = nn.functional.mse_loss(pred_delta, gt_delta)\n",
    "        \n",
    "        # Control Loss\n",
    "        base_states = base_cache_struct.ssm_states.detach().clone()\n",
    "        layers_list = [base_states[i] for i in range(model.config.num_hidden_layers)]\n",
    "        layers_list[LAYER_IDX] = layers_list[LAYER_IDX] + pred_delta\n",
    "        \n",
    "        class MockCache: # Re-defined for training\n",
    "            def __init__(self, ssm, conv): self.ssm_states, self.conv_states, self.config, self.conv_kernel_size = ssm, conv, model.config, model.config.conv_kernel\n",
    "            def update_ssm_state(self, *args, **kwargs): pass\n",
    "            def update_conv_state(self, *args, **kwargs): pass\n",
    "            \n",
    "        diff_cache = MockCache(torch.stack(layers_list), base_cache_struct.conv_states)\n",
    "        outputs = model(last_token_id, cache_params=diff_cache, cache_position=torch.tensor([context_ids.shape[1]], device=device))\n",
    "        l_control = nn.functional.cross_entropy(outputs.logits[0, -1].view(1, -1), torch.tensor([t_id], device=device))\n",
    "        \n",
    "        loss = l_control + 1.0 * l_match\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_l += loss.item()\n",
    "        \n",
    "    if epoch % 20 == 0: print(f\"Epoch {epoch}: Loss {total_l/len(dataset):.4f}\")\n",
    "\n",
    "# 4. Final Evaluation\n",
    "phi.eval()\n",
    "with torch.no_grad():\n",
    "    for sample in dataset:\n",
    "        t_embed = embedding_layer(torch.tensor([[sample['target_id']]], device=device)).view(1, -1)\n",
    "        pred_delta = phi(sample['h_prev'].to(device).view(1, -1), t_embed).view(sample_h.shape)\n",
    "        base_states = base_cache_struct.ssm_states.detach().clone()\n",
    "        layers_list = [base_states[i] for i in range(model.config.num_hidden_layers)]\n",
    "        layers_list[LAYER_IDX] = layers_list[LAYER_IDX] + pred_delta\n",
    "        diff_cache = MockCache(torch.stack(layers_list), base_cache_struct.conv_states)\n",
    "        logits = model(last_token_id, cache_params=diff_cache, cache_position=torch.tensor([context_ids.shape[1]], device=device)).logits[0, -1]\n",
    "        rank = (logits > logits[sample['target_id']]).sum().item() + 1\n",
    "        print(f\"Target: {sample['target_str']} | Final Rank: {rank}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3a: Zero-Shot Generalization (Unseen Tokens) ---\n",
      "Unseen Target: PINK     | Rank:  3 | Prob: 0.0919\n",
      "Unseen Target: CYAN     | Rank: 285 | Prob: 0.0000\n",
      "Unseen Target: BROWN    | Rank: 244 | Prob: 0.0000\n",
      "Unseen Target: NAVY     | Rank: 1148 | Prob: 0.0000\n",
      "Unseen Target: EMERALD  | Rank: 978 | Prob: 0.0000\n",
      "Unseen Target: MAROON   | Rank: 3042 | Prob: 0.0000\n",
      "\n",
      "--- 3b: Generation Stability Test ---\n",
      "Injected Target: BLUE\n",
      "Generated Text:   'I'm not sure I can do that.'\n",
      "\n",
      "'I'm not sure you can either,'\n"
     ]
    }
   ],
   "source": [
    "# --- PHASE 3: ROBUSTNESS & STABILITY (CORRECTED) ---\n",
    "phi.eval()\n",
    "\n",
    "# 1. Unseen Tokens (Zero-Shot)\n",
    "UNSEEN_TARGETS = [\"PINK\", \"CYAN\", \"BROWN\", \"NAVY\", \"EMERALD\", \"MAROON\"]\n",
    "print(\"--- 3a: Zero-Shot Generalization (Unseen Tokens) ---\")\n",
    "\n",
    "# Define h_prev globally from the dataset\n",
    "h_prev_global = dataset[0]['h_prev']\n",
    "\n",
    "with torch.no_grad():\n",
    "    for target_str in UNSEEN_TARGETS:\n",
    "        t_id = tokenizer.encode(target_str, add_special_tokens=False)[0]\n",
    "        t_embed = embedding_layer(torch.tensor([[t_id]], device=device)).view(1, -1)\n",
    "\n",
    "        # Predict delta for UNSEEN token\n",
    "        pred_delta = phi(h_prev_global.to(device).view(1, -1), t_embed).view(sample_h.shape)\n",
    "\n",
    "        # Inject\n",
    "        base_states = base_cache_struct.ssm_states.detach().clone()\n",
    "        layers_list = [base_states[i] for i in range(model.config.num_hidden_layers)]\n",
    "        layers_list[LAYER_IDX] = layers_list[LAYER_IDX] + pred_delta\n",
    "\n",
    "        diff_cache = MockCache(torch.stack(layers_list), base_cache_struct.conv_states)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(last_token_id, cache_params=diff_cache,\n",
    "                       cache_position=torch.tensor([context_ids.shape[1]], device=device)).logits[0, -1]\n",
    "\n",
    "        rank = (logits > logits[t_id]).sum().item() + 1\n",
    "        prob = torch.softmax(logits, dim=-1)[t_id].item()\n",
    "        print(f\"Unseen Target: {target_str:8} | Rank: {rank:2} | Prob: {prob:.4f}\")\n",
    "\n",
    "# 2. Generation Stability (The \"Continuation\" Test)\n",
    "print(\"\\n--- 3b: Generation Stability Test ---\")\n",
    "TARGET_TO_GENERATE = \"BLUE\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    t_id = tokenizer.encode(TARGET_TO_GENERATE, add_special_tokens=False)[0]\n",
    "    t_embed = embedding_layer(torch.tensor([[t_id]], device=device)).view(1, -1)\n",
    "\n",
    "    # Use h_prev_global\n",
    "    pred_delta = phi(h_prev_global.to(device).view(1, -1), t_embed).view(sample_h.shape)\n",
    "\n",
    "    base_states = base_cache_struct.ssm_states.detach().clone()\n",
    "    layers_list = [base_states[i] for i in range(model.config.num_hidden_layers)]\n",
    "    layers_list[LAYER_IDX] = layers_list[LAYER_IDX] + pred_delta\n",
    "\n",
    "    # We must use a real Mamba2Cache for generate() to work correctly\n",
    "    from transformers.models.mamba2.modeling_mamba2 import Mamba2Cache\n",
    "    gen_cache = Mamba2Cache(model.config, 1, device=device, dtype=model.dtype)\n",
    "    gen_cache.ssm_states = torch.stack(layers_list)\n",
    "    gen_cache.conv_states = base_cache_struct.conv_states.detach().clone()\n",
    "\n",
    "    # Generate starting from the last token of the probe (\"'\")\n",
    "    out = model.generate(input_ids=last_token_id, cache_params=gen_cache, max_new_tokens=20)\n",
    "    print(f\"Injected Target: {TARGET_TO_GENERATE}\")\n",
    "    print(f\"Generated Text:  {tokenizer.decode(out[0])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
